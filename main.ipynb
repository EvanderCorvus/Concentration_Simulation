{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concentration Gradient Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from environments import BoxEnvironment1 as env\n",
    "from environment_utils import Box, Circle2D\n",
    "from agents import SACAgent\n",
    "from agent_utils import update_target_agent, ReplayBuffer\n",
    "from log_utils import RLLogger\n",
    "from plot_utils import RLPlotter, plot_normalized_concentration, make_animation\n",
    "\n",
    "device = tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
    "tr.autograd.set_detect_anomaly(True)\n",
    "tr.set_default_tensor_type(tr.FloatTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Training -------------------\n",
    "    # Memory\n",
    "memory_size = 7000\n",
    "memory_batch_size = 512\n",
    "    # Duration of training\n",
    "runs = 1\n",
    "n_episodes = 50\n",
    "n_steps = 256\n",
    "    # Training parameters\n",
    "agent_batch_size = 128\n",
    "learning_rate_actor = 0.001\n",
    "learning_rate_critic = 0.0001\n",
    "milestones = np.arange(0, n_episodes, n_episodes)\n",
    "learing_rate_decay = 0.9\n",
    "\n",
    "entropy_coef = 0.01\n",
    "entropy_coef_decay = 0.95\n",
    "    # Bellman equation\n",
    "future_discount = 0.99\n",
    "    # Update Target Model\n",
    "target_model_update = 5\n",
    "    # Loss Function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# ---------------- Environment  ----------------\n",
    "    # Environment box size\n",
    "env_width = 2\n",
    "env_height = 2\n",
    "space = Box(env_width, env_height)\n",
    "    # Goal box size and center\n",
    "\n",
    "goal_radius = 0.1\n",
    "# goal_center = np.tile([0.5,0],(agent_batch_size,1))\n",
    "# goal = Box(goal_width, goal_height, goal_center)\n",
    "    # Time step size\n",
    "dt = 0.05\n",
    "    # Noise\n",
    "noise_characteristic_length = 3\n",
    "    # Maximum of potential\n",
    "c0 = 2\n",
    "\n",
    "# ---------------- Agent ----------------------\n",
    "state_dim = 3\n",
    "hidden_dims = [32,32,32]\n",
    "act_dim = 1\n",
    "act_positive = True\n",
    "act_scaling = 2*np.pi\n",
    "\n",
    "# ---------------- Other ----------------------\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.rcParams.update({'figure.dpi': 150})\n",
    "total_time = []\n",
    "update_state_time = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(space)\n",
    "memory = ReplayBuffer(state_dim, act_dim, memory_size, agent_batch_size)\n",
    "agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "target_agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "\n",
    "logger = RLLogger()\n",
    "plotter = RLPlotter(logger, 'logs')\n",
    "testLogger = RLLogger()\n",
    "testPlotter = RLPlotter(testLogger, 'test_logs', test=True)\n",
    "\n",
    "agent.actor_optimizer = tr.optim.Adam(agent.actor.parameters(), lr=learning_rate_actor)\n",
    "agent.critic1_optimizer = tr.optim.Adam(agent.critic1.parameters(), lr=learning_rate_critic)\n",
    "agent.critic2_optimizer = tr.optim.Adam(agent.critic2.parameters(), lr=learning_rate_critic)\n",
    "\n",
    "scheduler_actor = MultiStepLR(agent.actor_optimizer, milestones=milestones, gamma=learing_rate_decay)\n",
    "scheduler_critic1 = MultiStepLR(agent.critic1_optimizer, milestones=milestones, gamma=learing_rate_decay)\n",
    "scheduler_critic2 = MultiStepLR(agent.critic2_optimizer, milestones=milestones, gamma=learing_rate_decay)\n",
    "\n",
    "for p in target_agent.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, target_agent, memory_batch):\n",
    "    agent.critic1_optimizer.zero_grad()\n",
    "    agent.critic2_optimizer.zero_grad()\n",
    "\n",
    "    state_now = memory_batch['state_now'].reshape(-1, state_dim)\n",
    "    state_next = memory_batch['state_next'].reshape(-1, state_dim)\n",
    "    action_now = memory_batch['action_now'].reshape(-1, act_dim)\n",
    "    reward = memory_batch['reward'].reshape(-1)\n",
    "    done = memory_batch['done'].reshape(-1)\n",
    "    \n",
    "    # Compute Prediction\n",
    "    Q1_now_critic = agent.critic1(state_now, action_now)\n",
    "    Q2_now_critic = agent.critic2(state_now, action_now)\n",
    "\n",
    "    # Compute Target\n",
    "    with tr.no_grad():        \n",
    "        action_next_critic, log_prob_next_critic = agent.actor(state_next)\n",
    "        \n",
    "        Q1_next_critic = target_agent.critic1(state_next, action_next_critic)\n",
    "        Q2_next_critic = target_agent.critic2(state_next, action_next_critic)\n",
    "        Q_next_critic = tr.min(Q1_next_critic, Q2_next_critic)\n",
    "        target_critic = reward + future_discount*(1-done)*(Q_next_critic - entropy_coef*log_prob_next_critic)\n",
    "    # Compute Loss\n",
    "    loss_critic = loss_function(Q1_now_critic, target_critic) + loss_function(Q2_now_critic, target_critic)\n",
    "    \n",
    "    # Update\n",
    "    loss_critic.backward()\n",
    "    agent.critic1_optimizer.step()\n",
    "    agent.critic2_optimizer.step()\n",
    "    \n",
    "    agent.actor_optimizer.zero_grad()\n",
    "    for p in agent.critic1.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in agent.critic2.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    action_now_actor, log_prob_now_actor = agent.actor(state_now)\n",
    "    Q1_now_actor = agent.critic1(state_now, action_now_actor)\n",
    "    Q2_now_actor = agent.critic2(state_now, action_now_actor)\n",
    "    Q_now_actor = tr.min(Q1_now_actor, Q2_now_actor)\n",
    "    loss_actor = (entropy_coef*log_prob_now_actor - Q_now_actor).mean()\n",
    "    loss_actor.backward()\n",
    "    agent.actor_optimizer.step()\n",
    "\n",
    "    for p in agent.critic1.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in agent.critic2.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return loss_critic, loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():\n",
    "    # Initialize Goal at Random Location\n",
    "    # sample = space.sample()\n",
    "    sample = np.array([0.5,0])\n",
    "    goal_center = np.tile(sample,(agent_batch_size,1))\n",
    "    goal = Circle2D(goal_radius, goal_center)\n",
    "\n",
    "    environment.init_env(agent_batch_size, state_dim, goal, c0, random_start = False)\n",
    "    plotter.update_goal(goal)\n",
    "    goal_bool = False\n",
    "    for current_step in range(n_steps):\n",
    "        # Log state\n",
    "        logger.save_state(environment.state)\n",
    "        if current_step%target_model_update == 0 and current_step > memory_size:\n",
    "            update_target_agent(agent, target_agent)\n",
    "        # Beginning state\n",
    "        state_now = environment.state\n",
    "        # Action\n",
    "        # if memory.size < memory_batch_size:\n",
    "        #     action_now = 2*tr.pi*tr.rand(agent_batch_size, act_dim, device=device, dtype=tr.float)\n",
    "        # else:\n",
    "        action_now, _ = agent.actor(tr.as_tensor(environment.state, device=device, dtype=tr.float))\n",
    "        # Next state\n",
    "        reward = environment.step(action_now.detach().cpu().numpy(), c0, dt, noise_characteristic_length)\n",
    "        state_next = environment.state\n",
    "        # Done\n",
    "        done = environment.goal_check()\n",
    "        # Log action\n",
    "        logger.save_action(action_now.detach().cpu().numpy())\n",
    "\n",
    "        loss = 0\n",
    "        # Sample from memory\n",
    "        if memory.size >= memory_batch_size:\n",
    "            \n",
    "            memory_batch = memory.sample_batch(memory_batch_size)\n",
    "\n",
    "            # Update Agent\n",
    "            loss_critic, loss_actor = update(agent, target_agent, memory_batch)\n",
    "            loss_critic, loss_actor = loss_critic.item(), loss_actor.item()\n",
    "            logger.save_loss_critic(loss_critic)\n",
    "            logger.save_loss_actor(loss_actor)\n",
    "        \n",
    "        # Store in memory\n",
    "        memory.store((state_now[:,-1])[:,None], action_now, reward, (state_next[:,-1])[:,None], loss, done)\n",
    "        \n",
    "        if max(environment.goal_check()):\n",
    "            goal_bool = True\n",
    "        #     print('Goal reached')\n",
    "        #     logger.save_state(environment.state)\n",
    "            \n",
    "        #     break\n",
    "\n",
    "\n",
    "    return current_step, goal_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode():\n",
    "    # Initialize Goal at Random Location\n",
    "    # sample = space.sample()\n",
    "    sample = np.array([0.5,0])\n",
    "    goal_center = np.tile(sample,(1,1))\n",
    "    goal = Circle2D(goal_radius, goal_center)\n",
    "\n",
    "    environment.init_env(1, state_dim, goal, c0, random_start = False)\n",
    "    testPlotter.update_goal(goal)\n",
    "    testLogger.save_state(environment.state)\n",
    "    for current_step in range(n_steps):\n",
    "      \n",
    "        # Action\n",
    "        action_now = agent.act(tr.as_tensor(environment.state, device=device, dtype=tr.float), deterministic=True)\n",
    "        environment.step(action_now, c0, dt, noise_characteristic_length, test = True)\n",
    "\n",
    "        # Log Action and State\n",
    "        testLogger.save_action(action_now)\n",
    "        testLogger.save_state(environment.state)\n",
    "            \n",
    "    return current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached!\n",
      "0.0095\n",
      "Episode 0  finished!\n",
      "Goal reached!\n",
      "0.009025\n",
      "Episode 1  finished!\n",
      "Goal reached!\n",
      "0.00857375\n",
      "Episode 2  finished!\n",
      "Goal reached!\n",
      "0.0081450625\n",
      "Episode 3  finished!\n",
      "Goal reached!\n",
      "0.007737809374999999\n",
      "Episode 4  finished!\n",
      "Goal reached!\n",
      "0.007350918906249998\n",
      "Episode 5  finished!\n",
      "Goal reached!\n",
      "0.006983372960937498\n",
      "Episode 6  finished!\n",
      "Goal reached!\n",
      "0.006634204312890623\n",
      "Episode 7  finished!\n",
      "Goal reached!\n",
      "0.006302494097246091\n",
      "Episode 8  finished!\n",
      "Goal reached!\n",
      "0.005987369392383786\n",
      "Episode 9  finished!\n",
      "Goal reached!\n",
      "0.005688000922764597\n",
      "Episode 10  finished!\n",
      "Goal reached!\n",
      "0.005403600876626367\n",
      "Episode 11  finished!\n",
      "Goal reached!\n",
      "0.005133420832795048\n",
      "Episode 12  finished!\n",
      "Goal reached!\n",
      "0.0048767497911552955\n",
      "Episode 13  finished!\n",
      "Goal reached!\n",
      "0.00463291230159753\n",
      "Episode 14  finished!\n",
      "Goal reached!\n",
      "0.0044012666865176535\n",
      "Episode 15  finished!\n",
      "Goal reached!\n",
      "0.004181203352191771\n",
      "Episode 16  finished!\n",
      "Goal reached!\n",
      "0.003972143184582182\n",
      "Episode 17  finished!\n",
      "Goal reached!\n",
      "0.0037735360253530726\n",
      "Episode 18  finished!\n",
      "Goal reached!\n",
      "0.0035848592240854188\n",
      "Episode 19  finished!\n",
      "Goal reached!\n",
      "0.0034056162628811476\n",
      "Episode 20  finished!\n",
      "Goal reached!\n",
      "0.0032353354497370902\n",
      "Episode 21  finished!\n",
      "Goal reached!\n",
      "0.0030735686772502355\n",
      "Episode 22  finished!\n",
      "Goal reached!\n",
      "0.0029198902433877237\n",
      "Episode 23  finished!\n",
      "Goal reached!\n",
      "0.0027738957312183374\n",
      "Episode 24  finished!\n",
      "Goal reached!\n",
      "0.0026352009446574203\n",
      "Episode 25  finished!\n",
      "Goal reached!\n",
      "0.002503440897424549\n",
      "Episode 26  finished!\n",
      "Goal reached!\n",
      "0.0023782688525533216\n",
      "Episode 27  finished!\n",
      "Goal reached!\n",
      "0.0022593554099256553\n",
      "Episode 28  finished!\n",
      "Goal reached!\n",
      "0.0021463876394293723\n",
      "Episode 29  finished!\n",
      "Goal reached!\n",
      "0.0020390682574579037\n",
      "Episode 30  finished!\n",
      "Goal reached!\n",
      "0.0019371148445850085\n",
      "Episode 31  finished!\n",
      "Goal reached!\n",
      "0.0018402591023557579\n",
      "Episode 32  finished!\n",
      "Goal reached!\n",
      "0.0017482461472379698\n",
      "Episode 33  finished!\n",
      "Goal reached!\n",
      "0.0016608338398760713\n",
      "Episode 34  finished!\n",
      "Goal reached!\n",
      "0.0015777921478822676\n",
      "Episode 35  finished!\n",
      "Goal reached!\n",
      "0.001498902540488154\n",
      "Episode 36  finished!\n",
      "Goal reached!\n",
      "0.0014239574134637463\n",
      "Episode 37  finished!\n",
      "Goal reached!\n",
      "0.0013527595427905588\n",
      "Episode 38  finished!\n",
      "Goal reached!\n",
      "0.0012851215656510308\n",
      "Episode 39  finished!\n",
      "Goal reached!\n",
      "0.0012208654873684791\n",
      "Episode 40  finished!\n",
      "Goal reached!\n",
      "0.0011598222130000551\n",
      "Episode 41  finished!\n",
      "Goal reached!\n",
      "0.0011018311023500522\n",
      "Episode 42  finished!\n",
      "Goal reached!\n",
      "0.0010467395472325495\n",
      "Episode 43  finished!\n",
      "Goal reached!\n",
      "0.000994402569870922\n",
      "Episode 44  finished!\n",
      "Goal reached!\n",
      "0.0009446824413773759\n",
      "Episode 45  finished!\n",
      "Goal reached!\n",
      "0.0008974483193085071\n",
      "Episode 46  finished!\n",
      "Goal reached!\n",
      "0.0008525759033430817\n",
      "Episode 47  finished!\n",
      "Goal reached!\n",
      "0.0008099471081759276\n",
      "Episode 48  finished!\n",
      "Goal reached!\n",
      "0.0007694497527671312\n",
      "Episode 49  finished!\n"
     ]
    }
   ],
   "source": [
    "def simulation():\n",
    "    update_target_agent(agent, target_agent)\n",
    "    for ep in range(n_episodes):\n",
    "        # if ep%(n_episodes//10) == 0:\n",
    "        #     entropy_coef = entropy_coef * entropy_coef_decay\n",
    "        episode_steps, goal_bool = episode()\n",
    "        if goal_bool:\n",
    "            print('Goal reached!')\n",
    "            global entropy_coef\n",
    "            entropy_coef = entropy_coef * entropy_coef_decay\n",
    "            print(entropy_coef)\n",
    "\n",
    "        logger.save_episode(episode_steps)\n",
    "        plotter.plot_last_episode()\n",
    "        \n",
    "\n",
    "        test_episode_steps = test_episode()\n",
    "        testLogger.save_episode(test_episode_steps)\n",
    "        testPlotter.plot_last_episode()        \n",
    "        print('Episode', ep,' finished!')\n",
    "        if memory.size > memory_batch_size:\n",
    "            scheduler_actor.step()\n",
    "            scheduler_critic1.step()\n",
    "            scheduler_critic2.step()\n",
    "        \n",
    "plotter.clear_plots('logs')\n",
    "testPlotter.clear_plots('test_logs')\n",
    "\n",
    "simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video logs/episode_paths_animation.mp4.\n",
      "Moviepy - Writing video logs/episode_paths_animation.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready logs/episode_paths_animation.mp4\n",
      "Moviepy - Building video test_logs/episode_paths_animation.mp4.\n",
      "Moviepy - Writing video test_logs/episode_paths_animation.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test_logs/episode_paths_animation.mp4\n"
     ]
    }
   ],
   "source": [
    "make_animation('logs/episode_paths',3)\n",
    "make_animation('test_logs/episode_paths',3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
